import os
import sys
from typing import List, Optional
from contextlib import AsyncExitStack

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from langchain_ollama import ChatOllama
from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage, AIMessage
from config import logger

class LLMEngine:
    def __init__(self):
        self.session = Optional[ClientSession] = None
        self.exit_stack = AsyncExitStack()
        self.tool_definitions = []
        self.mcp_server_script = "mcp_server.py"
        
    async def connect_mcp(self):
        """MCP ì„œë²„ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì„œë¸Œ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰í•˜ì—¬ ì—°ê²°"""
        logger.info(f"MCP Server Process Connecting...")

        command = sys.executable
        script_path = os.path.abspath(self.mcp_server_script)

        server_params = StdioServerParameters(
            command = command,
            args=[script_path],
            env = os.environ.copy()
        )

        try:
            stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))
            self.session = await self.exit_stack.enter_async_context(
                ClientSession(stdio_transport[0], stdio_transport[1])
            )
            await self.session.initialize()

            response = await self.session.list_tools()
            self.tool_definitions = [
                {
                    "name" : tool.name,
                    "description" : tool.description,
                    "parameters" : tool.inputSchema
                }
                for tool in response.tools
            ]
            logger.info(f"LLM Engine ì¤€ë¹„ ì™„ë£Œ (Tools: {len(self.tool_definitions)}ê°œ)")
            return True
        except Exception as e:
            logger.error(f"MCP server Connect Fail: {e}")
            return False
    
    async def process_text(self, user_text: str) -> str:
        if not self.session:
            return "ì‹œìŠ¤í…œ ì˜¤ë¥˜: MCP ì—°ê²°ì´ ë˜ì–´ìˆì§€ ì•ŠìŠµë‹ˆë‹¤."

        llm = ChatOllama(
            model='gpt-oss:20b',
            temperature=0.1,
            num_ctx=4096
        )

        llm_with_tools = llm.bind_tools(self.tool_definitions)

        system_prompt = """ë‹¹ì‹ ì€ ìŠ¤ë§ˆíŠ¸í™ˆ AIì…ë‹ˆë‹¤.
        1. ì‚¬ìš©ìê°€ "ì–´ë‘¡ë‹¤", "ì¼œì¤˜"ë¼ê³  í•˜ë©´ ê´€ë ¨ ê¸°ê¸°ë¥¼ ì°¾ìœ¼ì„¸ìš”.
        2. `find_entity_by_name`ìœ¼ë¡œ ê¸°ê¸°ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        3. [ë§¤ìš° ì¤‘ìš”] ê²€ìƒ‰ ê²°ê³¼ì— ì „ë“±/ìŠ¤ìœ„ì¹˜ê°€ ì—¬ëŸ¬ ê°œ ë‚˜ì˜¤ë©´, ì ˆëŒ€ í•˜ë‚˜ì”© ì¼œì§€ ë§ˆì„¸ìš”.
        4. **ë°˜ë“œì‹œ `turn_on_multiple` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°œê²¬ëœ ëª¨ë“  ê¸°ê¸° IDë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ í•œ ë²ˆì— ì „ë‹¬í•˜ì„¸ìš”.**
        5. ë‹µë³€ì—ëŠ” ì´ëª¨ì§€(ğŸ˜Š, âœ… ë“±)ë¥¼ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.        
        """

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_text)
        ]

        logger.info(f"Thinking: {user_text}")
        final_answer = ""

        # ReAct ë£¨í”„
        for _ in range(10):
            response = await llm_with_tools.ainvoke(messages)
            messages.append(response)

            if not response.tool_calls:
                final_answer = response.content
                break
            
            for tool_call in response.tool_calls:
                t_name = tool_call["name"]
                t_args = tool_call["args"]
                t_id = tool_call["id"]

                logger.info(f"Tool Call: {t_name} {t_args}")

                try:
                    result = await self.session.call_tool(t_name, t_args)
                    result_text = result.content[0].text if result.content else str(result)
                except Exception as e:
                    result_text = f"Tool Error: {str(e)}"

                logger.info(f"Tool Result: {result_text}")
                messages.append(ToolMessage(content=result_text, tool_call_id =t_id))

        return final_answer if final_answer else "ì™„ë£Œí–ˆìŠµë‹ˆë‹¤."

    async def cleanup(self):
        await self.exit_stack.aclose()